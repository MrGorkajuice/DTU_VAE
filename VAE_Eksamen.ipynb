{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4TXWhiAGQVJ"
   },
   "source": [
    "2.1.1: Setup\n",
    "\n",
    "Importing various required modules and the MNIST dataset. Initializing randomizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEBI-teSY67-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torchvision.datasets import MNIST\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from torchvision.transforms import ToTensor\n",
    "# Static random seed\n",
    "np.random.seed(89)\n",
    "\n",
    "# Need to convert data to Tensor, because the DataLoader iterator refuses to work with PIL image objects.\n",
    "# Also experienced other trouble when having the PIL image\n",
    "pil2tensor = lambda x: ToTensor()(x).squeeze()   # ToTensor return (64,1,28,28), the squeeze() call removes the 1 dimension\n",
    "# Standard MNIST dataset (not binarized)\n",
    "mnist_train_data = MNIST(\"./temp/\", transform=pil2tensor, download=True, train=True)\n",
    "mnist_test_data = MNIST(\"./temp/\", transform=pil2tensor, download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxXpiIwXGYvY"
   },
   "source": [
    "2.1.2: Plot 8x8 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "ixxaXCQCO_dB",
    "outputId": "95e6471d-aa7d-460d-e80a-e9e0b9080403"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train_data, batch_size = 64)\n",
    "images, labels = next(iter(mnist_train_loader))\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10), squeeze=False)\n",
    "i = 0\n",
    "for ax in axs.flat:\n",
    "    ax.imshow(images[i], cmap='gray')\n",
    "    ax.set_title(\"%s\" % (labels[i].item()))\n",
    "    ax.axis('off')\n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAtN1IX7GhDi"
   },
   "source": [
    "2.1.3: Implement dynamic binarization\n",
    "\n",
    "Sampling binarized pixels using intensity as probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwTMdXXreBbl"
   },
   "outputs": [],
   "source": [
    "# Setup Bernoulli statistical sampling conversion from grey-scale to binary\n",
    "from torchvision import transforms\n",
    "\n",
    "# AXBR: bit wierd, I thought we would need to normalize the image to [0,1] like this:\n",
    "#      transforms.Lambda(lambda x: torch.bernoulli(x.double()/255))]))\n",
    "# because the MNIST image uses unsigned 8-bit, and thus have range 0-255. Does PyTorch datasets normalize when using __getitem__ ?\n",
    "# When using 'data[idx]' method the data comes out raw (0-255) without the transform being performed.\n",
    "# Seems the transform is only attached to the dataset, and first invoked when doing '__getitem__'.\n",
    "binarized_mnist_train_data = MNIST(\"./temp/\",\n",
    "                                   download=True,\n",
    "                                   train=True,\n",
    "                                   transform=transforms.Compose([pil2tensor,\n",
    "                                                                 transforms.Lambda(lambda x: torch.bernoulli(x))]))\n",
    "binarized_mnist_test_data = MNIST(\"./temp/\",\n",
    "                                  download=True,\n",
    "                                  train=False,\n",
    "                                  transform=transforms.Compose([pil2tensor,\n",
    "                                                                transforms.Lambda(lambda x: torch.bernoulli(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "_Qf9sLMqhyaQ",
    "outputId": "9701d2a4-a4fc-4f79-c854-0ed0fde5995c"
   },
   "outputs": [],
   "source": [
    "# Plot same image a couple of times to verify we are doing statistical sampling every time the image is drawn\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3), squeeze=False)\n",
    "for ax in axs.flat:\n",
    "    sample = binarized_mnist_train_data.__getitem__(22)[0]\n",
    "    assert torch.max(sample) == 1.0\n",
    "    assert torch.min(sample) == 0.0\n",
    "    ax.imshow(sample, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLIEWEXOoOpt"
   },
   "source": [
    "2.1.4: Plot binarized MNIST samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "-9Jj6BslTiPt",
    "outputId": "2c88f850-a28e-4b62-8280-10c5e8daa311"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "binarized_mnist_train_loader = DataLoader(binarized_mnist_train_data, batch_size = 64)\n",
    "images, labels = next(iter(binarized_mnist_train_loader))\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10), squeeze=False)\n",
    "i = 0\n",
    "for ax in axs.flat:\n",
    "    ax.imshow(images[i], cmap='gray')\n",
    "    ax.set_title(\"%s\" % (labels[i].item()))\n",
    "    ax.axis('off')\n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAfNjJ4vrJMq"
   },
   "source": [
    "2.2.1.1: VAE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "370tki2E3CBG"
   },
   "outputs": [],
   "source": [
    "# Implement reparameterized diagonal gaussian\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    def __init__(self, mu: Tensor, log_sigma: Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        return self.mu + self.sigma*self.sample_epsilon()\n",
    "        \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        from torch.distributions import Normal \n",
    "        return  Normal(loc=self.mu, scale=self.sigma).log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF5c_hYM8v-o"
   },
   "outputs": [],
   "source": [
    "# Return sum of values in all dimenensions, except the first one, which is assumed to be batch\n",
    "def reduce(x: Tensor) -> Tensor:\n",
    "    return x.view(x.size(0), -1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHWnmyLfrTBF"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "# Define hidden layer topology - list of sizes of hidden layers\n",
    "encoder_dimensions = [512, 256, 128]\n",
    "decoder_dimensions = [128, 256, 512]\n",
    "apply_per_layer_batchnorm = False\n",
    "\n",
    "# Implement VAE\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "\n",
    "        # Core parameters\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        #self.prior_params = torch.zeros(torch.Size([1, 2*latent_features]))\n",
    "        \n",
    "        # Dynamically constructing the encoder network\n",
    "        encoder_constructor = []\n",
    "        encoder_constructor.append(nn.Linear(in_features=self.observation_features, out_features=encoder_dimensions[0]))\n",
    "        encoder_constructor.append(nn.ReLU())\n",
    "        if apply_per_layer_batchnorm:\n",
    "            encoder_constructor.append(nn.BatchNorm1d(num_features=encoder_dimensions[0]))\n",
    "        for i in range(len(encoder_dimensions)-1):\n",
    "            encoder_constructor.append(nn.Linear(in_features=encoder_dimensions[i], out_features=encoder_dimensions[i+1]))\n",
    "            encoder_constructor.append(nn.ReLU())\n",
    "            if apply_per_layer_batchnorm:\n",
    "                encoder_constructor.append(nn.BatchNorm1d(num_features=encoder_dimensions[i+1]))\n",
    "        encoder_constructor.append(nn.Linear(in_features=encoder_dimensions[-1], out_features=2*self.latent_features))\n",
    "        self.encoder = nn.Sequential(*encoder_constructor)\n",
    "\n",
    "        # Dynamically constructing the decoder network\n",
    "        decoder_constructor = []\n",
    "        decoder_constructor.append(nn.Linear(in_features=self.latent_features, out_features=decoder_dimensions[0]))\n",
    "        decoder_constructor.append(nn.ReLU())\n",
    "        if apply_per_layer_batchnorm:\n",
    "            decoder_constructor.append(nn.BatchNorm1d(num_features=decoder_dimensions[0]))\n",
    "        for i in range(len(decoder_dimensions)-1):\n",
    "            decoder_constructor.append(nn.Linear(in_features=decoder_dimensions[i], out_features=decoder_dimensions[i+1]))\n",
    "            decoder_constructor.append(nn.ReLU())\n",
    "            if apply_per_layer_batchnorm:\n",
    "                decoder_constructor.append(nn.BatchNorm1d(num_features=decoder_dimensions[i+1]))\n",
    "        decoder_constructor.append(nn.Linear(in_features=decoder_dimensions[-1], out_features=self.observation_features))\n",
    "        self.decoder = nn.Sequential(*decoder_constructor)\n",
    "    \n",
    "    # Encode input into posterior distribution\n",
    "    def encode(self, x: Tensor) -> Distribution:\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    # Decode latent variables into reconstruction\n",
    "    def decode(self, z: Tensor) -> Distribution:\n",
    "        px_logits = self.decoder(z)\n",
    "        px_logits = px_logits.view(-1, *self.input_shape)\n",
    "        return Bernoulli(logits=px_logits)\n",
    "    \n",
    "    # Get the prior distribution\n",
    "    def prior(self, batch_size: int = 1) -> Distribution:\n",
    "        local_prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = local_prior_params.chunk(2, dim=-1)\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    # Sample from a provided distribution\n",
    "    def sample(self, distribution: ReparameterizedDiagonalGaussian) -> Tensor:\n",
    "        return distribution.rsample()\n",
    "    \n",
    "    # Compute the ELBO\n",
    "    def elbo(self, prior: Distribution, posterior: Distribution, reconstruction: Distribution, x: Tensor, z: Tensor) -> float:\n",
    "        tst = reconstruction.sample()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        log_px = reduce(reconstruction.log_prob(x))\n",
    "        log_pz = reduce(prior.log_prob(z))\n",
    "        log_qz = reduce(posterior.log_prob(z))\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl\n",
    "        return elbo\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Dict[str, Any]:\n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.encode(x)\n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.decode(z)\n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxhZisS33nwM",
    "outputId": "e40f6ce3-9970-43db-9b5f-8cea8c7e372e"
   },
   "outputs": [],
   "source": [
    "# Instantiate a VAE\n",
    "testVAE = VariationalAutoEncoder(sample.flatten().shape, 2)\n",
    "print(testVAE.encoder)\n",
    "print(testVAE.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xht6E_a-Mee"
   },
   "source": [
    "2.2.1.2: Print samples from untrained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Decoding sample from prior\n",
    "testVAE.cpu()\n",
    "testVAE.eval()\n",
    "\n",
    "prior = testVAE.prior(64)\n",
    "prior_sample = testVAE.sample(prior)\n",
    "decoded_prior_sample = testVAE.decode(prior_sample)\n",
    "sampled_decode_content = decoded_prior_sample.sample().view(64, 28, 28)\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10), squeeze=False)\n",
    "i = 0\n",
    "for ax in axs.flat:\n",
    "    ax.imshow(sampled_decode_content[i], cmap='gray')\n",
    "    ax.axis('off') \n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "cwWZmv9Y9Urq",
    "outputId": "a4b073a4-af3f-4872-fc57-3eb7afa0f798"
   },
   "outputs": [],
   "source": [
    "# Method 2: Reconstruction of input from binarized MNIST\n",
    "a = random.choices(binarized_mnist_train_data,k=64)\n",
    "b = [x[0] for x in a]\n",
    "img = torch.stack(b)\n",
    "sampled_decode_content = testVAE(img)['px'].sample().view(-1,28,28)\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10), squeeze=False)\n",
    "i = 0\n",
    "for ax in axs.flat:\n",
    "    ax.imshow(sampled_decode_content[i], cmap='gray')\n",
    "    ax.axis('off') \n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgglyZU6FNZl"
   },
   "source": [
    "2.2.1.3: Compute ELBO of 64 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pe-W5XaETD7",
    "outputId": "ba5de8d1-fe2d-4594-ca24-983f4859bd23"
   },
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(64, 784))\n",
    "labels = np.zeros(shape=(64, 1))\n",
    "for i in range(64):\n",
    "    sample = random.choice(binarized_mnist_train_data)\n",
    "    samples[i] = sample[0].view(1, -1).numpy()\n",
    "    labels[i] = sample[1]\n",
    "\n",
    "prior = testVAE.prior(64)\n",
    "samples_tensor = Tensor(samples)\n",
    "posterior = testVAE.encode(samples_tensor)\n",
    "z = testVAE.sample(posterior) # Random sampling\n",
    "reconstruction = testVAE.decode(z)\n",
    "elbo = testVAE.elbo(prior, posterior, reconstruction, samples_tensor, z) \n",
    "\n",
    "print(\"ELBO: \", elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.1 and 2.2.2.2: Implementing train and test methods in training helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class VAE_Trainer:\n",
    "    def __init__(self, network: VariationalAutoEncoder, train: DataLoader, test: DataLoader):\n",
    "        self.VAE = network\n",
    "        self.train_data = train\n",
    "        self.test_data = test\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "            network.cuda()\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "            network.cpu()\n",
    "        self.optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train(self):\n",
    "        self.VAE.train()\n",
    "        for images, labels in self.train_data:\n",
    "            images = images.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.VAE(images)\n",
    "            loss = -self.VAE.elbo(outputs['pz'], outputs['qz'], outputs['px'], images, outputs['z']).mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def test(self):\n",
    "        self.VAE.eval()\n",
    "        losses = []\n",
    "        i = 0\n",
    "        for images, labels in self.test_data:\n",
    "            images = images.to(self.device)\n",
    "            outputs = self.VAE(images)\n",
    "            elbos = self.VAE.elbo(outputs['pz'], outputs['qz'], outputs['px'], images, outputs['z']).cpu().detach().numpy()\n",
    "            losses = np.append(losses, elbos)\n",
    "        loss = -np.mean(losses)\n",
    "        print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.3: Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_mnist_test_loader = DataLoader(binarized_mnist_test_data, batch_size = 64)\n",
    "\n",
    "trainer = VAE_Trainer(testVAE, binarized_mnist_train_loader, binarized_mnist_test_loader)\n",
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"Training epoch \", i)\n",
    "    trainer.train()\n",
    "    print(\"Testing epoch \", i)\n",
    "    trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2.4: Generating samples from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testVAE.cpu()\n",
    "testVAE.eval()\n",
    "\n",
    "prior = testVAE.prior(64)\n",
    "prior_sample = testVAE.sample(prior)\n",
    "decoded_prior_sample = testVAE.decode(prior_sample)\n",
    "sampled_decode_content = decoded_prior_sample.sample().view(64, 28, 28)\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10), squeeze=False)\n",
    "i = 0\n",
    "for ax in axs.flat:\n",
    "    ax.imshow(sampled_decode_content[i], cmap='gray')\n",
    "    ax.axis('off') \n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pe-W5XaETD7",
    "outputId": "ba5de8d1-fe2d-4594-ca24-983f4859bd23"
   },
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(64, 784))\n",
    "labels = np.zeros(shape=(64, 1))\n",
    "for i in range(64):\n",
    "    sample = random.choice(binarized_mnist_train_data)\n",
    "    samples[i] = sample[0].view(1, -1).numpy()\n",
    "    labels[i] = sample[1]\n",
    "\n",
    "prior = testVAE.prior(64)\n",
    "samples_tensor = Tensor(samples)\n",
    "posterior = testVAE.encode(samples_tensor)\n",
    "z = testVAE.sample(posterior) # Random sampling\n",
    "reconstruction = testVAE.decode(z)\n",
    "elbo = testVAE.elbo(prior, posterior, reconstruction, samples_tensor, z) \n",
    "\n",
    "print(\"ELBO: \", elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VAE Eksamen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
